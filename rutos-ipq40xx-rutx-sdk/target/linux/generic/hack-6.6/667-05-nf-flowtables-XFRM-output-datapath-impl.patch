net: netfilter: flowtables: Add flowtable XFRM direct offload.

Currently, XFRM direct offload can compute ESP encapsulations without involving
any netfilter hooks and can make use of FLOW_OFFLOAD_XMIT_DIRECT to skip neigh
lookups and certain interfaces.

This does mean that the final ESP packets will bypass firewall rules, like they do
with esp{4,6}_offload modules. On most applications this should totally acceptable,
but there are cases where this might break stuff, eg. if you are relying on 
OUTPUT or POSTROUTING hooks to mangle your esp packets in some way. Note that
though SNAT uses POSTROUTING hook, SNAT should still work thanks to the use of
'nf_flow_xfrm_it_harder'.

This also means that XFRM policy lookups are cached per connection. Care should be taken
to flush conntrack table or XFRM states when XFRM policies change. Note that XFRM states
are checked per-packet and in the GC loop, so changing these should work as expected.

--- a/net/netfilter/Makefile
+++ b/net/netfilter/Makefile
@@ -145,6 +145,7 @@ nf_flow_table-objs		:= nf_flow_table_cor
 				   nf_flow_table_offload.o
 nf_flow_table-$(CONFIG_NF_FLOW_TABLE_PROCFS) += nf_flow_table_procfs.o
 nf_flow_table-$(CONFIG_XFRM)	+= nf_flow_table_xfrm.o
+nf_flow_table-$(CONFIG_NF_FLOW_TABLE_XFRM) += nf_flow_table_xfrm_output.o
 
 obj-$(CONFIG_NF_FLOW_TABLE_INET) += nf_flow_table_inet.o
 
--- /dev/null
+++ b/net/netfilter/nf_flow_table_xfrm_output.c
@@ -0,0 +1,716 @@
+// SPDX-License-Identifier: GPL-2.0-only
+#include <linux/kernel.h>
+#include <linux/netfilter.h>
+#include <linux/netdevice.h>
+#include <net/xfrm.h>
+#include <net/netfilter/nf_flow_table.h>
+#include <net/icmp.h>
+#include <net/inet_ecn.h>
+#include <net/esp.h>
+#include <crypto/aead.h>
+#include <net/netfilter/nf_conntrack.h>
+#include <net/gso.h>
+
+struct nf_flow_xfrm_skb_cb {
+	union {
+		struct xfrm_skb_cb cb;
+		struct xfrm_mode_skb_cb mode_cb;
+	} header;
+
+	struct flow_offload_tuple *tuple;
+};
+
+static inline struct nf_flow_xfrm_skb_cb *nf_flow_xfrm_cb(struct sk_buff *skb)
+{
+	return (struct nf_flow_xfrm_skb_cb *)skb->cb;
+}
+
+static inline struct flow_offload_tuple *nf_flow_xfrm_cb_tuple(struct sk_buff *skb)
+{
+	struct nf_flow_xfrm_skb_cb *cb = nf_flow_xfrm_cb(skb);
+
+	return cb->tuple;
+}
+
+static inline struct flow_offload *nf_flow_xfrm_cb_flow(struct sk_buff *skb)
+{
+	struct nf_flow_xfrm_skb_cb *cb = nf_flow_xfrm_cb(skb);
+	struct flow_offload_tuple *tuple = cb->tuple;
+	struct flow_offload_tuple_rhash *tuplehash = container_of(tuple, struct flow_offload_tuple_rhash, tuple);
+	u8 dir = tuple->dir;
+	struct flow_offload *flow = container_of(tuplehash, struct flow_offload, tuplehash[dir]);
+
+	return flow;
+}
+
+static void nf_flow_xfrm_cb_release(struct sk_buff *skb)
+{
+	struct flow_offload *flow = nf_flow_xfrm_cb_flow(skb);
+
+	nf_flow_offload_put(flow);
+}
+
+static int xfrm_skb_check_space(struct sk_buff *skb, struct flow_offload_tuple *tuple)
+{
+	struct dst_entry *dst = skb_dst(skb);
+	int nhead = dst->header_len + LL_RESERVED_SPACE(dst->dev)
+		- skb_headroom(skb);
+	int ntail = dst->dev->needed_tailroom - skb_tailroom(skb);
+
+	if (nhead <= 0) {
+		if (ntail <= 0)
+			return 0;
+		nhead = 0;
+	} else if (ntail < 0)
+		ntail = 0;
+
+	return pskb_expand_head(skb, nhead, ntail, GFP_ATOMIC);
+}
+
+/* derived from __ip_local_out */
+static int nf_flow_xfrm_dst_local_out_inet(struct sk_buff *skb)
+{
+	struct iphdr *iph = ip_hdr(skb);
+
+	iph->tot_len = htons(skb->len);
+	ip_send_check(iph);
+
+	skb->protocol = htons(ETH_P_IP);
+
+	return 0;
+}
+
+/* derived from __ip6_local_out */
+static int nf_flow_xfrm_dst_local_out_inet6(struct sk_buff *skb)
+{
+	int len;
+
+	len = skb->len - sizeof(struct ipv6hdr);
+	if (len > IPV6_MAXPLEN)
+		len = 0;
+	ipv6_hdr(skb)->payload_len = htons(len);
+	IP6CB(skb)->nhoff = offsetof(struct ipv6hdr, nexthdr);
+
+	skb->protocol = htons(ETH_P_IPV6);
+
+	return 0;
+}
+
+static int nf_flow_xfrm_dst_local_out(struct sk_buff *skb) {
+	switch (skb_dst(skb)->ops->family) {
+	case AF_INET:
+		return nf_flow_xfrm_dst_local_out_inet(skb);
+	case AF_INET6:
+		return nf_flow_xfrm_dst_local_out_inet6(skb);
+	default:
+		BUG();
+		return -ENOTSUPP;
+	}
+}
+
+static int nf_flow_xfrm_output_one(struct sk_buff *skb,
+				  struct flow_offload_tuple *tuple,
+				  struct xfrm_state *x);
+
+static int nf_flow_xfrm_output_xmit(struct sk_buff *skb)
+{
+	struct flow_offload *flow = nf_flow_xfrm_cb_flow(skb);
+	struct flow_offload_tuple *tuple = nf_flow_xfrm_cb_tuple(skb);
+	int ret;
+
+	switch (ntohs(skb->protocol)) {
+	case ETH_P_IP:
+		ret = nf_flow_offload_ip_hook_tail(skb, flow, tuple);
+		break;
+	case ETH_P_IPV6:
+		ret = nf_flow_offload_ipv6_hook_tail(skb, flow, tuple);
+		break;
+	default:
+		BUG();
+		ret = NF_DROP;
+	}
+
+	nf_flow_offload_put(flow);
+	if (ret == NF_DROP) {
+		kfree_skb(skb);
+		ret = NF_STOLEN;
+	}
+
+	return ret;
+}
+
+static inline bool nf_flow_xfrm_direct_eval(struct dst_entry* dst)
+{
+	struct xfrm_state *x;
+
+	x = dst->xfrm;
+	if (!x)
+		return false;
+
+	if (x->type->proto != IPPROTO_ESP)
+		return false;
+
+	if (x->props.mode != XFRM_MODE_TUNNEL &&
+	    x->props.mode != XFRM_MODE_TRANSPORT &&
+	    x->props.mode != XFRM_MODE_BEET)
+		return false;
+
+	if (x->encap)
+		return false;
+
+#ifdef CONFIG_NET_L3_MASTER_DEV
+	if (netif_is_l3_slave(dst->dev))
+		return false;
+#endif
+
+	return true;
+}
+
+/* derived from xfrm_output_one, `resume` label */
+static int nf_flow_xfrm_output_one_tail(struct sk_buff *skb,
+				       struct xfrm_state *x, int err)
+{
+	struct net *net = xs_net(x);
+	struct xfrm_dst *xdst = (struct xfrm_dst *)skb_dst(skb);
+	struct flow_offload_tuple *tuple = nf_flow_xfrm_cb_tuple(skb);
+	int ret;
+
+	if (err) {
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATEPROTOERROR);
+		goto drop;
+	}
+
+	// Pop dst. Don't take new refernce here,
+	// flow_offload is still holding ref to bundle root
+	skb_dst_set_noref(skb, xdst->child);
+
+	if (!nf_flow_xfrm_direct_eval(skb_dst(skb)) ||
+	    skb_dst(skb)->xfrm->outer_mode.flags & XFRM_MODE_FLAG_TUNNEL) {
+		ret = nf_flow_xfrm_dst_local_out(skb);
+		if (ret)
+			goto drop;
+	}
+
+	if (!nf_flow_xfrm_direct_eval(skb_dst(skb)))
+		return nf_flow_xfrm_output_xmit(skb);
+
+	return nf_flow_xfrm_output_one(skb, tuple, skb_dst(skb)->xfrm);
+drop:
+	nf_flow_xfrm_cb_release(skb);
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+struct esp_skb_cb {
+	struct xfrm_skb_cb xfrm;
+	void *tmp;
+};
+
+struct esp_output_extra {
+	__be32 seqhi;
+	u32 esphoff;
+};
+
+#define ESP_SKB_CB(__skb) ((struct esp_skb_cb *)&((__skb)->cb[0]))
+
+/*
+ * Allocate an AEAD request structure with extra space for SG and IV.
+ *
+ * For alignment considerations the IV is placed at the front, followed
+ * by the request and finally the SG list.
+ *
+ * TODO: Use spare space in skb for this where possible.
+ */
+static void *esp_alloc_tmp(struct crypto_aead *aead, int nfrags, int extralen)
+{
+	unsigned int len;
+
+	len = extralen;
+
+	len += crypto_aead_ivsize(aead);
+
+	if (len) {
+		len += crypto_aead_alignmask(aead) &
+		       ~(crypto_tfm_ctx_alignment() - 1);
+		len = ALIGN(len, crypto_tfm_ctx_alignment());
+	}
+
+	len += sizeof(struct aead_request) + crypto_aead_reqsize(aead);
+	len = ALIGN(len, __alignof__(struct scatterlist));
+
+	len += sizeof(struct scatterlist) * nfrags;
+
+	return kmalloc(len, GFP_ATOMIC);
+}
+
+static inline void *esp_tmp_extra(void *tmp)
+{
+	return PTR_ALIGN(tmp, __alignof__(struct esp_output_extra));
+}
+
+static inline u8 *esp_tmp_iv(struct crypto_aead *aead, void *tmp, int extralen)
+{
+	return crypto_aead_ivsize(aead) ?
+	       PTR_ALIGN((u8 *)tmp + extralen,
+			 crypto_aead_alignmask(aead) + 1) : tmp + extralen;
+}
+
+static inline struct aead_request *esp_tmp_req(struct crypto_aead *aead, u8 *iv)
+{
+	struct aead_request *req;
+
+	req = (void *)PTR_ALIGN(iv + crypto_aead_ivsize(aead),
+				crypto_tfm_ctx_alignment());
+	aead_request_set_tfm(req, aead);
+	return req;
+}
+
+static inline struct scatterlist *esp_req_sg(struct crypto_aead *aead,
+					     struct aead_request *req)
+{
+	return (void *)ALIGN((unsigned long)(req + 1) +
+			     crypto_aead_reqsize(aead),
+			     __alignof__(struct scatterlist));
+}
+
+static void esp_ssg_unref(struct xfrm_state *x, void *tmp)
+{
+	struct esp_output_extra *extra = esp_tmp_extra(tmp);
+	struct crypto_aead *aead = x->data;
+	int extralen = 0;
+	u8 *iv;
+	struct aead_request *req;
+	struct scatterlist *sg;
+
+	if (x->props.flags & XFRM_STATE_ESN)
+		extralen += sizeof(*extra);
+
+	extra = esp_tmp_extra(tmp);
+	iv = esp_tmp_iv(aead, tmp, extralen);
+	req = esp_tmp_req(aead, iv);
+
+	/* Unref skb_frag_pages in the src scatterlist if necessary.
+	 * Skip the first sg which comes from skb->data.
+	 */
+	if (req->src != req->dst)
+		for (sg = sg_next(req->src); sg; sg = sg_next(sg))
+			put_page(sg_page(sg));
+}
+
+/* Move ESP header back into place. */
+static void esp_restore_header(struct sk_buff *skb, unsigned int offset)
+{
+	struct ip_esp_hdr *esph = (void *)(skb->data + offset);
+	void *tmp = ESP_SKB_CB(skb)->tmp;
+	__be32 *seqhi = esp_tmp_extra(tmp);
+
+	esph->seq_no = esph->spi;
+	esph->spi = *seqhi;
+}
+
+static void esp_output_restore_header(struct sk_buff *skb)
+{
+	void *tmp = ESP_SKB_CB(skb)->tmp;
+	struct esp_output_extra *extra = esp_tmp_extra(tmp);
+
+	esp_restore_header(skb, skb_transport_offset(skb) + extra->esphoff -
+				sizeof(__be32));
+}
+
+static struct ip_esp_hdr *esp_output_set_extra(struct sk_buff *skb,
+					       struct xfrm_state *x,
+					       struct ip_esp_hdr *esph,
+					       struct esp_output_extra *extra)
+{
+	/* For ESN we move the header forward by 4 bytes to
+	 * accomodate the high bits.  We will move it back after
+	 * encryption.
+	 */
+	if ((x->props.flags & XFRM_STATE_ESN)) {
+		__u32 seqhi;
+		struct xfrm_offload *xo = xfrm_offload(skb);
+
+		if (xo)
+			seqhi = xo->seq.hi;
+		else
+			seqhi = XFRM_SKB_CB(skb)->seq.output.hi;
+
+		extra->esphoff = (unsigned char *)esph -
+				 skb_transport_header(skb);
+		esph = (struct ip_esp_hdr *)((unsigned char *)esph - 4);
+		extra->seqhi = esph->spi;
+		esph->seq_no = htonl(seqhi);
+	}
+
+	esph->spi = x->id.spi;
+
+	return esph;
+}
+
+/* derived from esp_output_done [esp4] */
+static void nf_flow_xfrm_esp_output_done(void *data, int err)
+{
+	struct sk_buff *skb = data;
+	void *tmp;
+	struct xfrm_state *x = skb_dst(skb)->xfrm;
+
+	tmp = ESP_SKB_CB(skb)->tmp;
+	esp_ssg_unref(x, tmp);
+	kfree(tmp);
+
+	nf_flow_xfrm_output_one_tail(skb, x, err);
+}
+
+/* derived from esp_output_done_esn [esp4] */
+static void nf_flow_xfrm_esp_output_done_esn(void *data, int err)
+{
+	struct sk_buff *skb = data;
+
+	esp_output_restore_header(skb);
+	nf_flow_xfrm_esp_output_done(data, err);
+}
+
+/* derived from esp_output_tail [esp4] */
+static int nf_flow_xfrm_esp_output_tail(struct xfrm_state *x, struct sk_buff *skb, struct esp_info *esp)
+{
+	u8 *iv;
+	int alen;
+	void *tmp;
+	int ivlen;
+	int assoclen;
+	int extralen;
+	struct page *page;
+	struct ip_esp_hdr *esph;
+	struct crypto_aead *aead;
+	struct aead_request *req;
+	struct scatterlist *sg, *dsg;
+	struct esp_output_extra *extra;
+	int err = -ENOMEM;
+
+	assoclen = sizeof(struct ip_esp_hdr);
+	extralen = 0;
+
+	if (x->props.flags & XFRM_STATE_ESN) {
+		extralen += sizeof(*extra);
+		assoclen += sizeof(__be32);
+	}
+
+	aead = x->data;
+	alen = crypto_aead_authsize(aead);
+	ivlen = crypto_aead_ivsize(aead);
+
+	tmp = esp_alloc_tmp(aead, esp->nfrags + 2, extralen);
+	if (!tmp)
+		goto error;
+
+	extra = esp_tmp_extra(tmp);
+	iv = esp_tmp_iv(aead, tmp, extralen);
+	req = esp_tmp_req(aead, iv);
+	sg = esp_req_sg(aead, req);
+
+	if (esp->inplace)
+		dsg = sg;
+	else
+		dsg = &sg[esp->nfrags];
+
+	esph = esp_output_set_extra(skb, x, esp->esph, extra);
+	esp->esph = esph;
+
+	sg_init_table(sg, esp->nfrags);
+	err = skb_to_sgvec(skb, sg,
+		           (unsigned char *)esph - skb->data,
+		           assoclen + ivlen + esp->clen + alen);
+	if (unlikely(err < 0))
+		goto error_free;
+
+	if (!esp->inplace) {
+		int allocsize;
+		struct page_frag *pfrag = &x->xfrag;
+
+		allocsize = ALIGN(skb->data_len, L1_CACHE_BYTES);
+
+		spin_lock_bh(&x->lock);
+		if (unlikely(!skb_page_frag_refill(allocsize, pfrag, GFP_ATOMIC))) {
+			spin_unlock_bh(&x->lock);
+			goto error_free;
+		}
+
+		skb_shinfo(skb)->nr_frags = 1;
+
+		page = pfrag->page;
+		get_page(page);
+		/* replace page frags in skb with new page */
+		__skb_fill_page_desc(skb, 0, page, pfrag->offset, skb->data_len);
+		pfrag->offset = pfrag->offset + allocsize;
+		spin_unlock_bh(&x->lock);
+
+		sg_init_table(dsg, skb_shinfo(skb)->nr_frags + 1);
+		err = skb_to_sgvec(skb, dsg,
+			           (unsigned char *)esph - skb->data,
+			           assoclen + ivlen + esp->clen + alen);
+		if (unlikely(err < 0))
+			goto error_free;
+	}
+
+	if ((x->props.flags & XFRM_STATE_ESN))
+		aead_request_set_callback(req, 0, nf_flow_xfrm_esp_output_done_esn, skb);
+	else
+		aead_request_set_callback(req, 0, nf_flow_xfrm_esp_output_done, skb);
+
+	aead_request_set_crypt(req, sg, dsg, ivlen + esp->clen, iv);
+	aead_request_set_ad(req, assoclen);
+
+	memset(iv, 0, ivlen);
+	memcpy(iv + ivlen - min(ivlen, 8), (u8 *)&esp->seqno + 8 - min(ivlen, 8),
+	       min(ivlen, 8));
+
+	ESP_SKB_CB(skb)->tmp = tmp;
+	err = crypto_aead_encrypt(req);
+
+	switch (err) {
+	case -EINPROGRESS:
+		goto error;
+
+	case -ENOSPC:
+		err = NET_XMIT_DROP;
+		break;
+
+	case 0:
+		if ((x->props.flags & XFRM_STATE_ESN))
+			esp_output_restore_header(skb);
+	}
+
+	if (sg != dsg)
+		esp_ssg_unref(x, tmp);
+
+error_free:
+	kfree(tmp);
+error:
+	return err;
+}
+
+/* derived from esp_output [esp4] */
+static int nf_flow_esp_output(struct sk_buff *skb, struct xfrm_state *x)
+{
+	int alen;
+	int blksize;
+	struct ip_esp_hdr *esph;
+	struct crypto_aead *aead;
+	struct esp_info esp;
+
+	esp.inplace = true;
+
+	esp.proto = *skb_mac_header(skb);
+	*skb_mac_header(skb) = IPPROTO_ESP;
+
+	/* skb is pure payload to encrypt */
+
+	aead = x->data;
+	alen = crypto_aead_authsize(aead);
+
+	esp.tfclen = 0;
+	if (x->tfcpad) {
+		struct xfrm_dst *dst = (struct xfrm_dst *)skb_dst(skb);
+		u32 padto;
+
+		padto = min(x->tfcpad, xfrm_state_mtu(x, dst->child_mtu_cached));
+		if (skb->len < padto)
+			esp.tfclen = padto - skb->len;
+	}
+
+	blksize = ALIGN(crypto_aead_blocksize(aead), 4);
+	esp.clen = ALIGN(skb->len + 2 + esp.tfclen, blksize);
+	esp.plen = esp.clen - skb->len - esp.tfclen;
+	esp.tailen = esp.tfclen + esp.plen + alen;
+
+	esp.esph = ip_esp_hdr(skb);
+
+	esp.nfrags = esp_output_head(x, skb, &esp);
+	if (esp.nfrags < 0)
+		return esp.nfrags;
+
+	esph = esp.esph;
+	esph->spi = x->id.spi;
+
+	esph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);
+	esp.seqno = cpu_to_be64(XFRM_SKB_CB(skb)->seq.output.low +
+				 ((u64)XFRM_SKB_CB(skb)->seq.output.hi << 32));
+
+	skb_push(skb, -skb_network_offset(skb));
+
+	return nf_flow_xfrm_esp_output_tail(x, skb, &esp);
+}
+
+static int nf_flow_xfrm_type_output(struct sk_buff *skb, struct xfrm_state *x)
+{
+	switch (x->type->proto) {
+	case IPPROTO_ESP:
+		return nf_flow_esp_output(skb, x);
+	default:
+		BUG();
+		return -ENOTSUPP;
+	}
+}
+
+/* derived from xfrm_output_one, inner loop */
+static int nf_flow_xfrm_output_one(struct sk_buff *skb,
+				  struct flow_offload_tuple *tuple,
+				  struct xfrm_state *x)
+{
+	struct net *net = xs_net(x);
+	struct flow_offload *flow = nf_flow_xfrm_cb_flow(skb);
+	int err;
+
+	skb->mark = xfrm_smark_get(skb->mark, x);
+
+	err = xfrm_outer_mode_output(x, skb);
+	if (err) {
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATEMODEERROR);
+		goto drop;
+	}
+
+	spin_lock_bh(&x->lock);
+
+	if (unlikely(x->km.state != XFRM_STATE_VALID)) {
+		flow_offload_teardown(flow);
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATEINVALID);
+		err = -EINVAL;
+		goto drop_unlock;
+	}
+
+	err = xfrm_state_check_expire(x);
+	if (err) {
+		flow_offload_teardown(flow);
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATEEXPIRED);
+		goto drop_unlock;
+	}
+
+	err = xfrm_replay_overflow(x, skb);
+	if (err) {
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTSTATESEQERROR);
+		goto drop_unlock;
+	}
+
+	x->curlft.bytes += skb->len;
+	x->curlft.packets++;
+
+	spin_unlock_bh(&x->lock);
+
+	/* Inner headers are invalid now. */
+	skb->encapsulation = 0;
+
+	err = nf_flow_xfrm_type_output(skb, x);
+	if (err == -EINPROGRESS)
+		goto out;
+
+	return nf_flow_xfrm_output_one_tail(skb, x, err);
+
+drop_unlock:
+	spin_unlock_bh(&x->lock);
+drop:
+	nf_flow_xfrm_cb_release(skb);
+	kfree_skb(skb);
+out:
+	return NF_STOLEN;
+}
+
+/* derived from xfrm_output_one, inner loop */
+static int nf_flow_xfrm_direct_one(struct sk_buff *skb,
+				  struct flow_offload_tuple *tuple,
+				  struct xfrm_state *x)
+{
+	struct net *net = xs_net(x);
+	int err;
+
+	err = xfrm_skb_check_space(skb, tuple);
+	if (err) {
+		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTERROR);
+		goto drop;
+	}
+
+	return nf_flow_xfrm_output_one(skb, tuple, x);
+
+drop:
+	nf_flow_xfrm_cb_release(skb);
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+/* derived from xfrm_output_gso */
+static int nf_flow_xfrm_direct_gso(struct sk_buff *skb, struct flow_offload *flow, int dir)
+{
+	struct sk_buff *segs, *nskb;
+	struct flow_offload_tuple *tuple = &flow->tuplehash[dir].tuple;
+	struct xfrm_state *x = tuple->xfrm_route.bundle->xfrm;
+	struct nf_flow_xfrm_skb_cb *cb;
+
+	BUILD_BUG_ON(sizeof(*IPCB(skb)) > SKB_GSO_CB_OFFSET);
+	BUILD_BUG_ON(sizeof(*IP6CB(skb)) > SKB_GSO_CB_OFFSET);
+	segs = skb_gso_segment(skb, 0);
+	kfree_skb(skb);
+	if (IS_ERR(segs))
+		return PTR_ERR(segs);
+	if (segs == NULL)
+		return -EINVAL;
+
+	skb_list_walk_safe(segs, segs, nskb) {
+		skb_mark_not_on_list(segs);
+
+		nf_flow_offload_hold(flow);
+
+		cb = nf_flow_xfrm_cb(segs);
+		cb->tuple = tuple;
+		skb_dst_set_noref(segs, tuple->xfrm_route.bundle);
+
+		nf_flow_xfrm_direct_one(segs, tuple, x);
+	}
+
+	return 0;
+}
+
+/* derived from xfrm_output */
+int nf_flow_xmit_xfrm_direct(struct sk_buff *skb, struct flow_offload *flow, int dir)
+{
+	struct flow_offload_tuple *tuple = &flow->tuplehash[dir].tuple;
+	struct nf_flow_xfrm_skb_cb *cb;
+	struct xfrm_state *x = tuple->xfrm_route.bundle->xfrm;
+	int err;
+
+	BUILD_BUG_ON(sizeof(struct nf_flow_xfrm_skb_cb) >
+		     sizeof_field(struct sk_buff, cb));
+
+	switch (x->outer_mode.family) {
+	case AF_INET:
+		memset(IPCB(skb), 0, sizeof(*IPCB(skb)));
+		IPCB(skb)->flags |= IPSKB_XFRM_TRANSFORMED;
+		break;
+	case AF_INET6:
+		memset(IP6CB(skb), 0, sizeof(*IP6CB(skb)));
+
+		IP6CB(skb)->flags |= IP6SKB_XFRM_TRANSFORMED;
+		break;
+	}
+
+	secpath_reset(skb);
+
+	if (skb_is_gso(skb)) {
+		nf_flow_xfrm_direct_gso(skb, flow, dir);
+		return NF_STOLEN;
+	}
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		err = skb_checksum_help(skb);
+		if (err) {
+			return NF_DROP;
+		}
+	}
+
+	nf_flow_offload_hold(flow);
+
+	cb = nf_flow_xfrm_cb(skb);
+	cb->tuple = tuple;
+	skb_dst_set_noref(skb, tuple->xfrm_route.bundle);
+
+	return nf_flow_xfrm_direct_one(skb, tuple, x);
+}
+EXPORT_SYMBOL_GPL(nf_flow_xmit_xfrm_direct);
--- a/include/net/netfilter/nf_flow_table_xfrm.h
+++ b/include/net/netfilter/nf_flow_table_xfrm.h
@@ -154,4 +154,12 @@ static inline int nf_flow_xfrm_it_harder
 }
 #endif
 
+#ifdef CONFIG_NF_FLOW_TABLE_XFRM
+int nf_flow_xmit_xfrm_direct(struct sk_buff *skb, struct flow_offload *flow, int dir);
+static inline bool nf_flow_route_has_bundle(struct flow_xfrm_route *xroute)
+{
+	return xroute->bundle != NULL;
+}
+#endif
+
 #endif /* _NF_FLOW_TABLE_XFRM_H */
--- a/net/netfilter/nf_flow_table_ip.c
+++ b/net/netfilter/nf_flow_table_ip.c
@@ -472,6 +472,11 @@ nf_flow_offload_ip_hook(void *priv, stru
 	else if (ret == 0)
 		return NF_ACCEPT;
 
+#ifdef CONFIG_NF_FLOW_TABLE_XFRM
+	if (nf_flow_route_has_bundle(&tuplehash->tuple.xfrm_route)) {
+		return nf_flow_xmit_xfrm_direct(skb, flow, dir);
+	}
+#endif
 	memset(skb->cb, 0, sizeof(struct inet_skb_parm));
 	IPCB(skb)->iif = skb->dev->ifindex;
 	IPCB(skb)->flags = IPSKB_FORWARDED;
@@ -778,6 +783,11 @@ nf_flow_offload_ipv6_hook(void *priv, st
 	else if (ret == 0)
 		return NF_ACCEPT;
 
+#ifdef CONFIG_NF_FLOW_TABLE_XFRM
+	if (nf_flow_route_has_bundle(&tuplehash->tuple.xfrm_route)) {
+		return nf_flow_xmit_xfrm_direct(skb, flow, dir);
+	}
+#endif
 	memset(skb->cb, 0, sizeof(struct inet6_skb_parm));
 	IP6CB(skb)->iif = skb->dev->ifindex;
 	IP6CB(skb)->flags = IP6SKB_FORWARDED;
